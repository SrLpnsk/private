{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преддипломное практическое задание \n",
    "### Студента Липинского Сергея"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блоки кода модели Сорокина с частичными доработками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import bisect\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "# import ujson as json\n",
    "\n",
    "import keras.layers as kl\n",
    "import keras.backend as kb\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#from read import extract_morpheme_type, read_BMES, read_splitted\n",
    "#from tabled_trie import make_trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_BMES(morphs, morph_types):\n",
    "    answer = []\n",
    "    for morph, morph_type in zip(morphs, morph_types):\n",
    "        if len(morph) == 1:\n",
    "            answer.append(\"S-\" + morph_type)\n",
    "        else:\n",
    "            answer.append(\"B-\" + morph_type)\n",
    "            answer.extend([\"M-\" + morph_type] * (len(morph) - 2))\n",
    "            answer.append(\"E-\" + morph_type)\n",
    "    return answer\n",
    "\n",
    "def extract_morpheme_type(x):\n",
    "    return x[2:].lower()\n",
    "\n",
    "def read_BMES(infile, transform_to_BMES=True, n=None,\n",
    "              morph_sep=\"/\" ,sep=\":\", shuffle=True, BMES_func = generate_BMES):\n",
    "    source, targets = [], []\n",
    "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            word, analysis = line.split(\"\\t\")\n",
    "            analysis = [x.split(sep) for x in analysis.split(morph_sep)]\n",
    "            morphs, morph_types = [elem[0] for elem in analysis], [elem[1] for elem in analysis]\n",
    "            target = BMES_func(morphs, morph_types) if transform_to_BMES else morphs\n",
    "            source.append(word)\n",
    "            targets.append(target)\n",
    "    indexes = list(range(len(source)))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indexes)\n",
    "    if n is not None:\n",
    "        indexes = indexes[:n]\n",
    "    source = [source[i] for i in indexes]\n",
    "    targets = [targets[i] for i in indexes]\n",
    "    return source, targets\n",
    "\n",
    "def read_splitted(infile, transform_to_BMES=True, n=None, morph_sep=\"/\", shuffle=True, BMES_func = generate_BMES):\n",
    "    source, targets = [], []\n",
    "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            word, analysis = line.split(\"\\t\")\n",
    "            morphs = analysis.split(morph_sep)\n",
    "            morph_types = [\"None\"] * len(morphs)\n",
    "            if transform_to_BMES:\n",
    "                target = BMES_func(morphs, morph_types)\n",
    "            else:\n",
    "                target = morph_types\n",
    "            source.append(word)\n",
    "            targets.append(target)\n",
    "    indexes = list(range(len(source)))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indexes)\n",
    "    if n is not None:\n",
    "        indexes = indexes[:n]\n",
    "    source = [source[i] for i in indexes]\n",
    "    targets = [targets[i] for i in indexes]\n",
    "    return source, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция generate_BMES_shrt, заменяющая исходную функцию generate_BMES модели. \n",
    "Обеим функциям на вход подаётся морфема и её тип, возвращают они список разметки каждого символа морфемы. \n",
    "В исходной модели используется BMES-разметка, где B -- начало морфемы, M -- середина, E -- конец морфемы, S -- разметка для морфемы из одного символа, таким образом каждый символ относится к одному из 4\\*4 + 2 + 2 = 20 классов, где 4 - количество BMES подклассов, 4 - типы \"длинных\" морфем (PREF, ROOT, SUFF, ENDING), 2 - типы морфемы POSTFIX (бывает только длиной в 2 символа), 2 - односимвольные типы POSTFIX и HYPH. \n",
    "Идея функции generate_BMES_shrt в том, чтобы сократить количество классов, исключив один из BMES классов (\"B\" или \"E\"), так как без разметки \"B\" начало новой морфемы в слове всегда можно отследить по предыдущей разметке \"E\" или \"S\". Помимо этого, символы POSTFIX также помечаются одним подклассом, так как постфиксов в русском языке всего два: -ся, -сь. Таким образом, количество классов снижается до 4\\*3 + 1 + 2 = 14 классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У функции read_BMES (в предыдущем блоке) появился новый параметр generate_BMES, который позволяет размечать слова как полным, так и сокращённым вариантами BMES разметки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_BMES_shrt(morphs, morph_types):\n",
    "    answer = []\n",
    "#    without_start=True\n",
    "    long_morphs = set(['PREF', 'ROOT', 'SUFF', 'END'])\n",
    "#    if not(without_start):\n",
    "#        for morph, morph_type in zip(morphs, morph_types):\n",
    "#            if morph_type in long_morphs and len(morph) > 1:\n",
    "#                answer.append(\"B-\" + morph_type)\n",
    "#                answer.extend([\"M-\" + morph_type] * (len(morph) - 1))\n",
    "#            else:\n",
    "#                answer.append(\"S-\" + morph_type)\n",
    "#                if morph_type == 'POSTFIX':\n",
    "#                    answer.append(\"S-\" + morph_type)\n",
    "#    else:\n",
    "    for morph, morph_type in zip(morphs, morph_types):\n",
    "        if morph_type in long_morphs and len(morph) > 1:\n",
    "            answer.extend([\"M-\" + morph_type] * (len(morph) - 1))\n",
    "            answer.append(\"E-\" + morph_type)\n",
    "        else:\n",
    "            answer.append(\"S-\" + morph_type)\n",
    "            if morph_type == 'POSTFIX':\n",
    "                answer.append(\"S-\" + morph_type)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример работы функции generate BMES: читаем из тестового блока словаря Тихонова в двух режимах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = read_BMES(cur_dir+params[\"test_file\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, targets2 = read_BMES(cur_dir+params[\"test_file\"], shuffle=False, BMES_func = generate_BMES_shrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "воскрыляться\n",
      "['B-PREF', 'M-PREF', 'E-PREF', 'B-ROOT', 'M-ROOT', 'M-ROOT', 'E-ROOT', 'S-SUFF', 'B-SUFF', 'E-SUFF', 'B-POSTFIX', 'E-POSTFIX']\n",
      "['M-PREF', 'M-PREF', 'E-PREF', 'M-ROOT', 'M-ROOT', 'M-ROOT', 'E-ROOT', 'S-SUFF', 'M-SUFF', 'E-SUFF', 'S-POSTFIX', 'S-POSTFIX']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[2])\n",
    "print(targets[2])\n",
    "print(targets2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основной код модели с небольшими изменениями. \n",
    "В двух следующих блоках кода объявляется класс модели и внутренние функции, а также прочие вспомогательные функции (такие как сохранение модели в файл). Внесены следующие изменения: добавлена функция _decode_as_is, используемая вместо функции _decode_best. \n",
    "\n",
    "Функция _decode_best представляет из себя алгоритм, который улучшает предикт модели на основе некоторых жёстких логических правил, известных из морфологии. В том случае, если предикт модели получился невозможным (к примеру, после корня идёт приставка), функция исправляет эти неточности. \n",
    "\n",
    "На данном этапе для сравнения производительности одной и той же модели, обученной на двух способах разметки нам необходимо получить чистый предикт модели, без использования сторонних алгоритмов, поэтому добавлена функция _decode_as_is, которая возвращает вероятности на входе практически без изменений, но в нужном формате. Функция _predict_probs обзавелась параметром mode_func_appl, который осуществляет предсказание с использованием описанного алгоритма или без него (мы будем использовать обученные модели без него). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trie(words, alphabet=None, compressed=True, is_numpied=False,\n",
    "              make_cashed=False, precompute_symbols=False,\n",
    "              allow_spaces=False, dict_storage=False):\n",
    "    if alphabet is None:\n",
    "        alphabet = sorted({x for word in words for x in word})\n",
    "    trie = Trie(alphabet, is_numpied=is_numpied, to_make_cashed=make_cashed,\n",
    "                precompute_symbols=precompute_symbols, dict_storage=dict_storage)\n",
    "    trie.fit(words)\n",
    "    print(len(trie))\n",
    "    if compressed:\n",
    "        tm = TrieMinimizer()\n",
    "        trie = tm.minimize(trie, dict_storage=dict_storage, make_cashed=make_cashed,\n",
    "                           make_numpied=is_numpied, precompute_symbols=precompute_symbols,\n",
    "                           allow_spaces=allow_spaces)\n",
    "        print(len(trie))\n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(infile):\n",
    "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
    "        config = json.load(fin)\n",
    "    if \"use_morpheme_types\" not in config:\n",
    "        config[\"use_morpheme_types\"] = True\n",
    "    return config\n",
    "\n",
    "# вспомогательные фунцкии\n",
    "\n",
    "def to_one_hot(data, classes_number):\n",
    "    answer = np.eye(classes_number, dtype=np.uint8)\n",
    "    return answer[data]\n",
    "\n",
    "def make_model_file(name, i):\n",
    "    pos = name.rfind(\".\")\n",
    "    if pos != -1:\n",
    "        return \"{}-{}.{}\".format(name[:pos], i, name[pos+1:])\n",
    "    else:\n",
    "        return \"{}-{}\".format(name, i)\n",
    "\n",
    "\n",
    "AUXILIARY_CODES = PAD, BEGIN, END, UNKNOWN = 0, 1, 2, 3\n",
    "AUXILIARY = ['PAD', 'BEGIN', 'END', 'UNKNOWN']\n",
    "#AUXILIARY_CODES = PAD, BEGIN, END = 0, 1, 2\n",
    "#AUXILIARY = ['PAD', 'BEGIN', 'END']\n",
    "\n",
    "\n",
    "def _make_vocabulary(source):\n",
    "    \"\"\"\n",
    "    Создаёт словарь символов.\n",
    "    \"\"\"\n",
    "    symbols = {a for word in source for a in word}\n",
    "    symbols = AUXILIARY + sorted(symbols)\n",
    "    symbol_codes = {a: i for i, a in enumerate(symbols)}\n",
    "    return symbols, symbol_codes\n",
    "\n",
    "def make_bucket_lengths(lengths, buckets_number):\n",
    "    \"\"\"\n",
    "    Вычисляет максимальные длины элементов в корзинках. Каждая корзина состоит из элементов примерно одинаковой длины\n",
    "    \"\"\"\n",
    "    m = len(lengths)\n",
    "    lengths = sorted(lengths)\n",
    "    last_bucket_length, bucket_lengths = 0, []\n",
    "    for i in range(buckets_number):\n",
    "        # могут быть проблемы с выбросами большой длины\n",
    "        level = (m * (i + 1) // buckets_number) - 1\n",
    "        curr_length = lengths[level]\n",
    "        if curr_length > last_bucket_length:\n",
    "            bucket_lengths.append(curr_length)\n",
    "            last_bucket_length = curr_length\n",
    "    return bucket_lengths\n",
    "\n",
    "def collect_buckets(lengths, buckets_number, max_bucket_size=-1):\n",
    "    \"\"\"\n",
    "    Распределяет элементы по корзинам\n",
    "    \"\"\"\n",
    "    bucket_lengths = make_bucket_lengths(lengths, buckets_number)\n",
    "    indexes = [[] for _ in bucket_lengths]\n",
    "    for i, length in enumerate(lengths):\n",
    "        index = bisect.bisect_left(bucket_lengths, length)\n",
    "        indexes[index].append(i)\n",
    "    if max_bucket_size != -1:\n",
    "        bucket_lengths = list(chain.from_iterable(\n",
    "            ([L] * ((len(curr_indexes)-1) // max_bucket_size + 1))\n",
    "            for L, curr_indexes in zip(bucket_lengths, indexes)\n",
    "            if len(curr_indexes) > 0))\n",
    "        indexes = [curr_indexes[start:start+max_bucket_size]\n",
    "                   for curr_indexes in indexes\n",
    "                   for start in range(0, len(curr_indexes), max_bucket_size)]\n",
    "    return [(L, curr_indexes) for L, curr_indexes\n",
    "            in zip(bucket_lengths, indexes) if len(curr_indexes) > 0]\n",
    "\n",
    "def load_cls(infile):\n",
    "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
    "        json_data = json.load(fin)\n",
    "    args = {key: value for key, value in json_data.items()\n",
    "            if not (key.endswith(\"_\") or key.endswith(\"callback\") or key == \"model_files\")}\n",
    "    args['callbacks'] = []\n",
    "    # создаём классификатор\n",
    "    inflector = Partitioner(**args)\n",
    "    # обучаемые параметры\n",
    "    args = {key: value for key, value in json_data.items() if key[-1] == \"_\"}\n",
    "    for key, value in args.items():\n",
    "        setattr(inflector, key, value)\n",
    "    if hasattr(inflector, \"morphemes_\"):\n",
    "        inflector._make_morpheme_tries()\n",
    "    # модель\n",
    "    inflector.build()  # не работает сохранение/загрузка модели, приходится перекомпилировать\n",
    "    for i, (model, model_file) in enumerate(\n",
    "            zip(inflector.models_, json_data['model_files'])):\n",
    "        model.load_weights(model_file)\n",
    "    return inflector\n",
    "\n",
    "\n",
    "MORPHEME_TYPES = [\"PREF\", \"ROOT\", \"LINK\", \"END\", \"POSTFIX\", \"HYPH\"]\n",
    "PREF, ROOT, LINK, SUFF, ENDING, POSTFIX, HYPH, FINAL = 0, 1, 2, 3, 4, 5, 6, 7\n",
    "\n",
    "\n",
    "def get_next_morpheme_types(morpheme_type):\n",
    "    \"\"\"\n",
    "    Определяет, какие морфемы могут идти за текущей.\n",
    "    \"\"\"\n",
    "    if morpheme_type == \"None\":\n",
    "        return [\"None\"]\n",
    "    MORPHEMES = [\"SUFF\", \"END\", \"LINK\", \"POSTFIX\", \"PREF\", \"ROOT\"]\n",
    "    if morpheme_type in [\"ROOT\", \"SUFF\", \"HYPH\"]:\n",
    "        start = 0\n",
    "    elif morpheme_type == \"END\":\n",
    "        start = 2\n",
    "    elif morpheme_type in [\"PREF\", \"LINK\", \"BEGIN\"]:\n",
    "        start = 4\n",
    "    else:\n",
    "        start = 6\n",
    "    answer = MORPHEMES[start:6]\n",
    "    if len(answer) > 0 and morpheme_type != \"HYPH\":\n",
    "        answer.append(\"HYPH\")\n",
    "    if morpheme_type == \"BEGIN\":\n",
    "        answer.append(\"None\")\n",
    "    return answer\n",
    "\n",
    "def get_next_morpheme(morpheme):\n",
    "    \"\"\"\n",
    "    Строит список меток, которые могут идти за текущей\n",
    "    \"\"\"\n",
    "    if morpheme == \"BEGIN\":\n",
    "        morpheme = \"S-BEGIN\"\n",
    "    morpheme_label, morpheme_type = morpheme.split(\"-\")\n",
    "    if morpheme_label in \"BM\":\n",
    "        new_morpheme_labels = \"ME\"\n",
    "        new_morpheme_types = [morpheme_type]\n",
    "    else:\n",
    "        new_morpheme_labels = \"BS\"\n",
    "        new_morpheme_types = get_next_morpheme_types(morpheme_type)\n",
    "    answer = [\"{}-{}\".format(x, y) for x in new_morpheme_labels for y in new_morpheme_types]\n",
    "    return answer\n",
    "\n",
    "\n",
    "def is_correct_morpheme_sequence(morphemes):\n",
    "    \"\"\"\n",
    "    Проверяет список морфемных меток на корректность\n",
    "    \"\"\"\n",
    "    if morphemes == []:\n",
    "        return False\n",
    "    if any(\"-\" not in morpheme for morpheme in morphemes):\n",
    "        return False\n",
    "    morpheme_label, morpheme_type = morphemes[0].split(\"-\")\n",
    "    if morpheme_label not in \"BS\" or morpheme_type not in [\"PREF\", \"ROOT\", \"None\"]:\n",
    "        return False\n",
    "    morpheme_label, morpheme_type = morphemes[-1].split(\"-\")\n",
    "    if morpheme_label not in \"ES\" or morpheme_type not in [\"ROOT\", \"SUFF\", \"ENDING\", \"POSTFIX\", \"None\"]:\n",
    "        return False\n",
    "    for i, morpheme in enumerate(morphemes[:-1]):\n",
    "        if morphemes[i+1] not in get_next_morpheme(morpheme):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "class Partitioner:\n",
    "\n",
    "    \"\"\"\n",
    "    models_number: int, default=1, число моделей\n",
    "    to_memorize_morphemes: bool, default=False,\n",
    "        производится ли запоминание морфемных энграмм\n",
    "    min_morpheme_count: int, default=2,\n",
    "        минимальное количество раз, которое должна встречаться запоминаемая морфема\n",
    "    to_memorize_ngram_counts: bool, default=False,\n",
    "        используются ли частоты энграмм как морфем при вычислении признаков\n",
    "    min_relative_ngram_count: float, default=0.1,\n",
    "        минимальное отношение частоты энграммы как морфемы к её общей частоте,\n",
    "        необходимое для её запоминания\n",
    "    use_embeddings: bool, default=False,\n",
    "        используется ли дополнительный слой векторных представлений символов\n",
    "    embeddings_size: int, default=32, размер символьного представления\n",
    "    conv_layers: int, default=1, число свёрточных слоёв\n",
    "    window_size: int or list of ints, список размеров окна в свёрточном слое\n",
    "    filters_number: int or list of ints or list of list of ints,\n",
    "        число фильтров в свёрточных слоях,\n",
    "        filters_number[i,j] --- число фильтров для i-го окна j-го слоя,\n",
    "        если задан список, то filters_number[j] --- число фильтров в окнах j-го слоя,\n",
    "        если число --- то одно и то же число фильтров для всех слоёв и окон\n",
    "    dense_output_units: int, default=0,\n",
    "        число нейронов на дополнительном слое перед вычислением выходных вероятностей.\n",
    "        если 0, то этот слой отсутствует\n",
    "    use_lstm: bool, default=False,\n",
    "        используется ли дополнительный выходной слой LSTM (ухудшает качество)\n",
    "    lstm_units: int, default=64, число нейронов в LSTM-слое\n",
    "    dropout: float, default=0.0\n",
    "        доля выкидываемых нейронов в dropout-слое, помогает бороться с переобучением\n",
    "    context_dropout: float, default=0.0,\n",
    "        вероятность маскировки векторного представления контекста\n",
    "    buckets_number: int, default=10,\n",
    "        число корзин, в одну корзину попадают данные примерно одинаковой длины\n",
    "    nepochs: int, default=10, число эпох в обучении\n",
    "    validation_split: float, default=0.2, доля элементов в развивающей выборке\n",
    "    batch_size: int, default=32, число элементов в одном батче\n",
    "    callbacks: list of keras.callbacks or None, default=None,\n",
    "        коллбэки для управления процессом обучения,\n",
    "    early_stopping: int, default=None,\n",
    "        число эпох, в течение которого не должно улучшаться качество\n",
    "        на валидационной выборке, чтобы обучение остановилось,\n",
    "        если None, то в любом случае модель обучается nepochs эпох\n",
    "    \"\"\"\n",
    "\n",
    "    LEFT_MORPHEME_TYPES = [\"pref\", \"root\"]\n",
    "    RIGHT_MORPHEME_TYPES = [\"root\", \"suff\", \"end\", \"postfix\"]\n",
    "\n",
    "    def __init__(self, models_number=1, use_morpheme_types=True,\n",
    "                 to_memorize_morphemes=False, min_morpheme_count=2,\n",
    "                 to_memorize_ngram_counts=False, min_relative_ngram_count=0.1,\n",
    "                 use_embeddings=False, embeddings_size=32,\n",
    "                 conv_layers=1, window_size=5, filters_number=64,\n",
    "                 dense_output_units=0, use_lstm=False, lstm_units=64,\n",
    "                 dropout=0.0, context_dropout=0.0,\n",
    "                 buckets_number=10, nepochs=10,\n",
    "                 validation_split=0.2, batch_size=32,\n",
    "                 callbacks=None, early_stopping=None):\n",
    "        self.models_number = models_number\n",
    "        self.use_morpheme_types = use_morpheme_types\n",
    "        self.to_memorize_morphemes = to_memorize_morphemes\n",
    "        self.min_morpheme_count = min_morpheme_count\n",
    "        self.to_memorize_ngram_counts = to_memorize_ngram_counts\n",
    "        self.min_relative_ngram_count = min_relative_ngram_count\n",
    "        self.use_embeddings = use_embeddings\n",
    "        self.embeddings_size = embeddings_size\n",
    "        self.conv_layers = conv_layers\n",
    "        self.window_size = window_size\n",
    "        self.filters_number = filters_number\n",
    "        self.dense_output_units = dense_output_units\n",
    "        self.use_lstm = use_lstm\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dropout = dropout\n",
    "        self.context_dropout = context_dropout\n",
    "        self.buckets_number = buckets_number\n",
    "        self.nepochs = nepochs\n",
    "        self.validation_split = validation_split\n",
    "        self.batch_size = batch_size\n",
    "        self.callbacks = callbacks\n",
    "        self.early_stopping = early_stopping\n",
    "        self.check_params()\n",
    "\n",
    "    def check_params(self):\n",
    "        if isinstance(self.window_size, int):\n",
    "            # если было только одно окно в свёрточных слоях\n",
    "            self.window_size = [self.window_size]\n",
    "        # приводим фильтры к двумерному виду\n",
    "        self.filters_number = np.atleast_2d(self.filters_number)\n",
    "        if self.filters_number.shape[0] == 1:\n",
    "            self.filters_number = np.repeat(self.filters_number, len(self.window_size), axis=0)\n",
    "        if self.filters_number.shape[0] != len(self.window_size):\n",
    "            raise ValueError(\"Filters array should have shape (len(window_size), conv_layers)\")\n",
    "        if self.filters_number.shape[1] == 1:\n",
    "            self.filters_number = np.repeat(self.filters_number, self.conv_layers, axis=1)\n",
    "        if self.filters_number.shape[1] != self.conv_layers:\n",
    "            raise ValueError(\"Filters array should have shape (len(window_size), conv_layers)\")\n",
    "        # переводим в список из int, а не np.int32, чтобы не было проблем при сохранении\n",
    "        self.filters_number = list([list(map(int, x)) for x in self.filters_number])\n",
    "        if self.callbacks is None:\n",
    "            self.callbacks = []\n",
    "        if (self.early_stopping is not None and\n",
    "                not any(isinstance(x, EarlyStopping) for x in self.callbacks)):\n",
    "            self.callbacks.append(EarlyStopping(patience=self.early_stopping, monitor=\"val_acc\"))\n",
    "        if self.use_morpheme_types:\n",
    "            self._morpheme_memo_func = self._make_morpheme_data\n",
    "        else:\n",
    "            self._morpheme_memo_func = self._make_morpheme_data_simple\n",
    "\n",
    "    def to_json(self, outfile, model_file=None):\n",
    "        info = dict()\n",
    "        if model_file is None:\n",
    "            pos = outfile.rfind(\".\")\n",
    "            model_file = outfile[:pos] + (\"-model.hdf5\" if pos != -1 else \"-model\")\n",
    "        model_files = [make_model_file(model_file, i+1) for i in range(self.models_number)]\n",
    "        for i in range(self.models_number):\n",
    "            # при сохранении нужен абсолютный путь, а не от текущей директории\n",
    "            model_files[i] = os.path.abspath(model_files[i])\n",
    "        for (attr, val) in inspect.getmembers(self):\n",
    "            # перебираем поля класса и сохраняем только задаваемые при инициализации\n",
    "            if not (attr.startswith(\"__\") or inspect.ismethod(val) or\n",
    "                    isinstance(getattr(Partitioner, attr, None), property) or\n",
    "                    attr.isupper() or attr in [\n",
    "                        \"callbacks\", \"models_\", \"left_morphemes_\", \"right_morphemes_\", \"morpheme_trie_\"]):\n",
    "                info[attr] = val\n",
    "            elif attr == \"models_\":\n",
    "                # для каждой модели сохраняем веса\n",
    "                info[\"model_files\"] = model_files\n",
    "                for model, curr_model_file in zip(self.models_, model_files):\n",
    "                    model.save_weights(curr_model_file)\n",
    "        with open(outfile, \"w\", encoding=\"utf8\") as fout:\n",
    "            json.dump(info, fout)\n",
    "\n",
    "    # property --- функция, прикидывающаяся переменной; декоратор метода (превращает метод класса в атрибут класса)\n",
    "    @property \n",
    "    def symbols_number_(self):\n",
    "        return len(self.symbols_)\n",
    "\n",
    "    @property\n",
    "    def target_symbols_number_(self):\n",
    "        return len(self.target_symbols_)\n",
    "\n",
    "    @property\n",
    "    def memory_dim(self):\n",
    "        return 15 if self.use_morpheme_types else 3\n",
    "\n",
    "    def _preprocess(self, data, targets=None):\n",
    "        # к каждому слову добавляются символы начала и конца строки\n",
    "        lengths = [len(x) + 2 for x in data]\n",
    "        # разбиваем данные на корзины\n",
    "        buckets_with_indexes = collect_buckets(lengths, self.buckets_number)\n",
    "        # преобразуем данные в матрицы в каждой корзине\n",
    "        data_by_buckets = [self._make_bucket_data(data, length, indexes)\n",
    "                           for length, indexes in buckets_with_indexes]\n",
    "        # targets=None --- предсказание, иначе --- обучение\n",
    "        if targets is not None:\n",
    "            targets_by_buckets = [self._make_bucket_data(targets, length, indexes, is_target=True)\n",
    "                                  for length, indexes in buckets_with_indexes]\n",
    "            return data_by_buckets, targets_by_buckets, buckets_with_indexes\n",
    "        else:\n",
    "            return data_by_buckets, buckets_with_indexes\n",
    "\n",
    "    def _make_bucket_data(self, data, bucket_length, bucket_indexes, is_target=False):\n",
    "        \"\"\"\n",
    "        data: list of lists, исходные данные\n",
    "        bucket_length: int, максимальная длина элемента в корзине\n",
    "        bucket_indexes: list of ints, индексы элементов в корзине\n",
    "        is_target: boolean, default=False,\n",
    "            являются ли данные исходными или ответами\n",
    "\n",
    "        answer = [symbols, (classes)],\n",
    "            symbols: array of shape (len(data), bucket_length)\n",
    "                элементы data, дополненные символом PAD справа до bucket_length\n",
    "            classes: array of shape (len(data), classes_number)\n",
    "        \"\"\"\n",
    "        bucket_data = [data[i] for i in bucket_indexes]\n",
    "        if is_target:\n",
    "            return self._recode_bucket_data(bucket_data, bucket_length, self.target_symbol_codes_)\n",
    "        else:\n",
    "            answer = [self._recode_bucket_data(bucket_data, bucket_length, self.symbol_codes_)]\n",
    "            if self.to_memorize_morphemes:\n",
    "                print(\"Processing morphemes for bucket length\", bucket_length)\n",
    "                answer.append(self._morpheme_memo_func(bucket_data, bucket_length))\n",
    "                print(\"Processing morphemes for bucket length\", bucket_length, \"finished\")\n",
    "            return answer\n",
    "\n",
    "    def _recode_bucket_data(self, data, bucket_length, encoding):\n",
    "        answer = np.full(shape=(len(data), bucket_length), fill_value=PAD, dtype=int)\n",
    "        answer[:,0] = BEGIN\n",
    "        for j, word in enumerate(data):\n",
    "            answer[j,1:1+len(word)] = [encoding.get(x, UNKNOWN) for x in word]\n",
    "            answer[j,1+len(word)] = END\n",
    "        return answer\n",
    "\n",
    "    def _make_morpheme_data(self, data, bucket_length):\n",
    "        \"\"\"\n",
    "        строит для каждой позиции во входных словах вектор, кодирующий энграммы в контексте\n",
    "\n",
    "        data: list of strs, список исходных слов\n",
    "        bucket_length: int, максимальная длина слова в корзине\n",
    "\n",
    "        answer: np.array[float] of shape (len(data), bucket_length, 15)\n",
    "        \"\"\"\n",
    "        answer = np.zeros(shape=(len(data), bucket_length, 15), dtype=float)\n",
    "        for j, word in enumerate(data):\n",
    "            m = len(word)\n",
    "            curr_answer = np.zeros(shape=(bucket_length, 15), dtype=int)\n",
    "            root_starts = [0]\n",
    "            ending_ends = [m]\n",
    "            prefixes = self.left_morphemes_[\"pref\"].descend_by_prefixes(word[:-1])\n",
    "            for end in prefixes:\n",
    "                score = self._get_ngram_score(word[:end], \"pref\")\n",
    "                if end == 1:\n",
    "                    curr_answer[1,10] = max(score, curr_answer[1,10])\n",
    "                else:\n",
    "                    curr_answer[1,0] = max(score, curr_answer[1,0])\n",
    "                    curr_answer[end, 5] = max(score, curr_answer[end, 5])\n",
    "            root_starts += prefixes\n",
    "            postfix_lengths = self.right_morphemes_[\"postfix\"].descend_by_prefixes(word[:0:-1])\n",
    "            for k in postfix_lengths:\n",
    "                score = self._get_ngram_score(word[-k:], \"postfix\")\n",
    "                if k == 1:\n",
    "                    curr_answer[m, 14] = max(score, curr_answer[m, 14])\n",
    "                else:\n",
    "                    curr_answer[m, 9] = max(score, curr_answer[m, 9])\n",
    "                    curr_answer[m-k+1,4] = max(score, curr_answer[m-k+1,4])\n",
    "                ending_ends.append(m-k)\n",
    "            suffix_ends = set(ending_ends)\n",
    "            for end in ending_ends[::-1]:\n",
    "                ending_lengths = self.right_morphemes_[\"end\"].descend_by_prefixes(word[end-1:0:-1])\n",
    "                for k in ending_lengths:\n",
    "                    score = self._get_ngram_score(word[end-k:end], \"end\")\n",
    "                    if k == 1:\n",
    "                        curr_answer[end, 13] = max(score, curr_answer[end, 13])\n",
    "                    else:\n",
    "                        curr_answer[end-k+1, 3] = max(score, curr_answer[end-k+1, 3])\n",
    "                        curr_answer[end, 8] = max(score, curr_answer[end, 8])\n",
    "                    suffix_ends.add(end-k)\n",
    "            suffixes = self.right_morphemes_[\"suff\"].descend_by_prefixes(\n",
    "                word[::-1], start_pos=[m-k for k in suffix_ends], max_count=3, return_pairs=True)\n",
    "            suffix_starts = suffix_ends\n",
    "            for first, last in suffixes:\n",
    "                score = self._get_ngram_score(word[m-last:m-first], \"suff\")\n",
    "                if last == first + 1:\n",
    "                    curr_answer[m-first, 12] = max(score, curr_answer[m-first, 12])\n",
    "                else:\n",
    "                    curr_answer[m-last+1, 2] = max(score, curr_answer[m-last+1, 2])\n",
    "                    curr_answer[m-first, 7] = max(score, curr_answer[m-first, 7])\n",
    "                suffix_starts.add(m-last)\n",
    "            for start in root_starts:\n",
    "                root_ends = self.left_morphemes_[\"root\"].descend_by_prefixes(word[start:])\n",
    "                for end in root_ends:\n",
    "                    score = self._get_ngram_score(word[start:end], \"root\")\n",
    "                    if end == start+1:\n",
    "                        curr_answer[start + 1, 11] = max(score, curr_answer[start + 1, 11])\n",
    "                    else:\n",
    "                        curr_answer[start + 1, 1] = max(score, curr_answer[start + 1, 1])\n",
    "                        curr_answer[end, 6] = max(score, curr_answer[end, 6])\n",
    "            for end in suffix_starts:\n",
    "                root_lengths = self.right_morphemes_[\"root\"].descend_by_prefixes(word[end-1:-1:-1])\n",
    "                for k in root_lengths:\n",
    "                    score = self._get_ngram_score(word[end-k:end], 'root')\n",
    "                    if k == 1:\n",
    "                        curr_answer[end, 11] = max(curr_answer[end, 11], score)\n",
    "                    else:\n",
    "                        curr_answer[end-k+1, 1] = max(curr_answer[end-k+1, 1], score)\n",
    "                        curr_answer[end, 6] = max(curr_answer[end, 6], score)\n",
    "            answer[j] = curr_answer\n",
    "        return answer\n",
    "\n",
    "    def _make_morpheme_data_simple(self, data, bucket_length):\n",
    "        answer = np.zeros(shape=(len(data), bucket_length, 3), dtype=float)\n",
    "        for j, word in enumerate(data):\n",
    "            m = len(word)\n",
    "            curr_answer = np.zeros(shape=(bucket_length, 3), dtype=int)\n",
    "            positions = self.morpheme_trie_.find_substrings(word, return_positions=True)\n",
    "            for starts, end in positions:\n",
    "                for start in starts:\n",
    "                    score = self._get_ngram_score(word[start:end])\n",
    "                    if end == start+1:\n",
    "                        curr_answer[start+1, 2] = max(curr_answer[start+1, 2], score)\n",
    "                    else:\n",
    "                        curr_answer[start+1, 0] = max(curr_answer[start+0, 2], score)\n",
    "                        curr_answer[end, 1] = max(curr_answer[end, 1], score)\n",
    "            answer[j] = curr_answer\n",
    "        return answer\n",
    "\n",
    "    def _get_ngram_score(self, ngram, mode=\"None\"):\n",
    "        if self.to_memorize_ngram_counts:\n",
    "            return self.morpheme_counts_[mode].get(ngram, 0)\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    def train(self, source, targets, dev=None, dev_targets=None, model_file=None):\n",
    "        \"\"\"\n",
    "\n",
    "        source: list of strs, список слов для морфемоделения\n",
    "        targets: list of strs, метки морфемоделения в формате BMES\n",
    "        model_file: str or None, default=None, файл для сохранения моделей\n",
    "\n",
    "        Возвращает:\n",
    "        -------------\n",
    "        self, обученный морфемоделитель\n",
    "        \"\"\"\n",
    "        self.symbols_, self.symbol_codes_ = _make_vocabulary(source)\n",
    "        self.target_symbols_, self.target_symbol_codes_ = _make_vocabulary(targets)\n",
    "        if self.to_memorize_morphemes:\n",
    "            self._memorize_morphemes(source, targets)\n",
    "\n",
    "        data_by_buckets, targets_by_buckets, _ = self._preprocess(source, targets)\n",
    "        if dev is not None:\n",
    "            dev_data_by_buckets, dev_targets_by_buckets, _ = self._preprocess(dev, dev_targets)\n",
    "        else:\n",
    "            dev_data_by_buckets, dev_targets_by_buckets = None, None\n",
    "        self.build()\n",
    "        self._train_models(data_by_buckets, targets_by_buckets,  dev_data_by_buckets,\n",
    "                           dev_targets_by_buckets, model_file=model_file)\n",
    "        return self\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Создаёт нейронные модели\n",
    "        \"\"\"\n",
    "        self.models_ = [self.build_model() for _ in range(self.models_number)]\n",
    "        print(self.models_[0].summary())\n",
    "        return self\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Функция, задающая архитектуру нейронной сети\n",
    "        \"\"\"\n",
    "        # symbol_inputs: array, 1D-массив длины m\n",
    "        symbol_inputs = kl.Input(shape=(None,), dtype='uint8', name=\"symbol_inputs\")\n",
    "        # symbol_embeddings: array, 2D-массив размера m*self.symbols_number\n",
    "        if self.use_embeddings:\n",
    "            symbol_embeddings = kl.Embedding(self.symbols_number_, self.embeddings_size,\n",
    "                                             name=\"symbol_embeddings\")(symbol_inputs)\n",
    "        else:\n",
    "            symbol_embeddings = kl.Lambda(kb.one_hot, output_shape=(None, self.symbols_number_),\n",
    "                                          arguments={\"num_classes\": self.symbols_number_},\n",
    "                                          name=\"symbol_embeddings\")(symbol_inputs)\n",
    "        inputs = [symbol_inputs]\n",
    "        if self.to_memorize_morphemes:\n",
    "            # context_inputs: array, 2D-массив размера m*15\n",
    "            context_inputs = kl.Input(shape=(None, self.memory_dim), dtype='float32', name=\"context_inputs\")\n",
    "            inputs.append(context_inputs)\n",
    "            if self.context_dropout > 0.0:\n",
    "                context_inputs = kl.Dropout(self.context_dropout)(context_inputs)\n",
    "            # представление контекста подклеивается к представлению символа\n",
    "            symbol_embeddings = kl.Concatenate()([symbol_embeddings, context_inputs])\n",
    "        conv_inputs = symbol_embeddings\n",
    "        conv_outputs = []\n",
    "        for window_size, curr_filters_numbers in zip(self.window_size, self.filters_number):\n",
    "            # свёрточный слой отдельно для каждой ширины окна\n",
    "            curr_conv_input = conv_inputs\n",
    "            for j, filters_number in enumerate(curr_filters_numbers[:-1]):\n",
    "                # все слои свёртки, кроме финального (после них возможен dropout)\n",
    "                curr_conv_input = kl.Conv1D(filters_number, window_size,\n",
    "                                            activation=\"relu\", padding=\"same\")(curr_conv_input)\n",
    "                if self.dropout > 0.0:\n",
    "                    # между однотипными слоями рекомендуется вставить dropout\n",
    "                    curr_conv_input = kl.Dropout(self.dropout)(curr_conv_input)\n",
    "            if not self.use_lstm:\n",
    "                curr_conv_output = kl.Conv1D(curr_filters_numbers[-1], window_size,\n",
    "                                             activation=\"relu\", padding=\"same\")(curr_conv_input)\n",
    "            else:\n",
    "                curr_conv_output = curr_conv_input\n",
    "            conv_outputs.append(curr_conv_output)\n",
    "        # соединяем выходы всех свёрточных слоёв в один вектор\n",
    "        if len(conv_outputs) == 1:\n",
    "            conv_output = conv_outputs[0]\n",
    "        else:\n",
    "            conv_output = kl.Concatenate(name=\"conv_output\")(conv_outputs)\n",
    "        if self.use_lstm:\n",
    "            conv_output = kl.Bidirectional(\n",
    "                kl.LSTM(self.lstm_units, return_sequences=True))(conv_output)\n",
    "        if self.dense_output_units:\n",
    "            pre_last_output = kl.TimeDistributed(\n",
    "                kl.Dense(self.dense_output_units, activation=\"relu\"),\n",
    "                name=\"pre_output\")(conv_output)\n",
    "        else:\n",
    "            pre_last_output = conv_output\n",
    "        # финальный слой с softmax-активацией, чтобы получить распределение вероятностей\n",
    "        output = kl.TimeDistributed(\n",
    "            kl.Dense(self.target_symbols_number_, activation=\"softmax\"), name=\"output\")(pre_last_output)\n",
    "        model = Model(inputs, [output])\n",
    "        model.compile(optimizer=adam(clipnorm=5.0),\n",
    "                      loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def _train_models(self, data_by_buckets, targets_by_buckets,\n",
    "                      dev_data_by_buckets=None, dev_targets_by_buckets=None, model_file=None):\n",
    "        \"\"\"\n",
    "        data_by_buckets: list of lists of np.arrays,\n",
    "            data_by_buckets[i] = [..., bucket_i, ...],\n",
    "            bucket = [input_1, ..., input_k],\n",
    "            input_j --- j-ый вход нейронной сети, вычисленный для текущей корзины\n",
    "        targets_by_buckets: list of np.arrays,\n",
    "            targets_by_buckets[i] --- закодированные ответы для i-ой корзины\n",
    "        model_file: str or None, путь к файлу для сохранения модели\n",
    "        \"\"\"\n",
    "        train_indexes_by_buckets, dev_indexes_by_buckets = [], []\n",
    "        if dev_data_by_buckets is not None:\n",
    "            train_indexes_by_buckets = [list(range(len(bucket[0]))) for bucket in data_by_buckets]\n",
    "            for elem in train_indexes_by_buckets:\n",
    "                np.random.shuffle(elem)\n",
    "            dev_indexes_by_buckets = [list(range(len(bucket[0]))) for bucket in dev_data_by_buckets]\n",
    "            train_data, dev_data = data_by_buckets, dev_data_by_buckets\n",
    "            train_targets, dev_targets = targets_by_buckets, dev_targets_by_buckets\n",
    "        else:\n",
    "            for bucket in data_by_buckets:\n",
    "                # разбиваем каждую корзину на обучающую и валидационную выборку\n",
    "                L = len(bucket[0])\n",
    "                indexes_for_bucket = list(range(L))\n",
    "                np.random.shuffle(indexes_for_bucket)\n",
    "                train_bucket_length = int(L*(1.0 - self.validation_split))\n",
    "                train_indexes_by_buckets.append(indexes_for_bucket[:train_bucket_length])\n",
    "                dev_indexes_by_buckets.append(indexes_for_bucket[train_bucket_length:])\n",
    "            train_data, dev_data = data_by_buckets, data_by_buckets\n",
    "            train_targets, dev_targets = targets_by_buckets, targets_by_buckets\n",
    "        # разбиваем на батчи обучающую и валидационную выборку\n",
    "        # (для валидационной этого можно не делать, а подавать сразу корзины)\n",
    "        train_batches_indexes = list(chain.from_iterable(\n",
    "            [[(i, elem[j:j+self.batch_size]) for j in range(0, len(elem), self.batch_size)]\n",
    "             for i, elem in enumerate(train_indexes_by_buckets)]))\n",
    "        dev_batches_indexes = list(chain.from_iterable(\n",
    "            [[(i, elem[j:j+self.batch_size]) for j in range(0, len(elem), self.batch_size)]\n",
    "             for i, elem in enumerate(dev_indexes_by_buckets)]))\n",
    "        # поскольку функции fit_generator нужен генератор, порождающий batch за batch'ем,\n",
    "        # то приходится заводить генераторы для обеих выборок\n",
    "        train_gen = generate_data(train_data, train_targets, train_batches_indexes,\n",
    "                                  classes_number=self.target_symbols_number_, shuffle=True)\n",
    "        val_gen = generate_data(dev_data, dev_targets, dev_batches_indexes,\n",
    "                                classes_number=self.target_symbols_number_, shuffle=False)\n",
    "        for i, model in enumerate(self.models_):\n",
    "            if model_file is not None:\n",
    "                curr_model_file = make_model_file(model_file, i+1)\n",
    "                # для сохранения модели с наилучшим результатом на валидационной выборке\n",
    "                save_callback = ModelCheckpoint(curr_model_file, save_weights_only=True, save_best_only=True)\n",
    "                curr_callbacks = self.callbacks + [save_callback]\n",
    "            else:\n",
    "                curr_callbacks = self.callbacks\n",
    "            model.fit_generator(train_gen, len(train_batches_indexes),\n",
    "                                epochs=self.nepochs, callbacks=curr_callbacks,\n",
    "                                validation_data=val_gen, validation_steps=len(dev_batches_indexes))\n",
    "            if model_file is not None:\n",
    "                model.load_weights(curr_model_file)\n",
    "        return self\n",
    "\n",
    "    def _memorize_morphemes(self, words, targets):\n",
    "        \"\"\"\n",
    "        запоминает морфемы. встречающиеся в словах обучающей выборки\n",
    "        \"\"\"\n",
    "        morphemes = defaultdict(lambda: defaultdict(int))\n",
    "        for word, target in zip(words, targets):\n",
    "            start = None\n",
    "            for i, (symbol, label) in enumerate(zip(word, target)):\n",
    "                if label.startswith(\"B-\"):\n",
    "                    start = i\n",
    "                elif label.startswith(\"E-\"):\n",
    "                    dest = extract_morpheme_type(label)\n",
    "                    morphemes[dest][word[start:i+1]] += 1\n",
    "                elif label.startswith(\"S-\"):\n",
    "                    dest = extract_morpheme_type(label)\n",
    "                    morphemes[dest][word[i]] += 1\n",
    "                elif label == END:\n",
    "                    break\n",
    "        self.morphemes_ = dict()\n",
    "        for key, counts in morphemes.items():\n",
    "            self.morphemes_[key] = [x for x, count in counts.items() if count >= self.min_morpheme_count]\n",
    "        self._make_morpheme_tries()\n",
    "        if self.to_memorize_ngram_counts:\n",
    "            self._memorize_ngram_counts(words, morphemes)\n",
    "        return self\n",
    "\n",
    "    def _memorize_ngram_counts(self, words, counts):\n",
    "        \"\"\"\n",
    "        запоминает частоты морфем, встречающихся в словах обучающей выборки\n",
    "        \"\"\"\n",
    "        prefix_counts, suffix_counts, ngram_counts  = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "        for i, word in enumerate(words, 1):\n",
    "            if i % 5000 == 0:\n",
    "                print(\"{} words processed\".format(i))\n",
    "            positions = self.morpheme_trie_.find_substrings(word, return_positions=True)\n",
    "            for starts, end in positions:\n",
    "                for start in starts:\n",
    "                    segment = word[start:end]\n",
    "                    ngram_counts[segment] += 1\n",
    "                    if start == 0:\n",
    "                        prefix_counts[segment] += 1\n",
    "                    if end == len(word):\n",
    "                        suffix_counts[segment] += 1\n",
    "        self.morpheme_counts_ = dict()\n",
    "        for key, curr_counts in counts.items():\n",
    "            curr_relative_counts = dict()\n",
    "            curr_ngram_counts = (prefix_counts if key == \"pref\" else\n",
    "                                 suffix_counts if key in [\"end\", \"postfix\"] else ngram_counts)\n",
    "            for ngram, count in curr_counts.items():\n",
    "                if count < self.min_morpheme_count or ngram not in curr_ngram_counts:\n",
    "                    continue\n",
    "                relative_count = min(count / curr_ngram_counts[ngram], 1.0)\n",
    "                if relative_count >= self.min_relative_ngram_count:\n",
    "                    curr_relative_counts[ngram] = relative_count\n",
    "            self.morpheme_counts_[key] = curr_relative_counts\n",
    "        return self\n",
    "\n",
    "    def _make_morpheme_tries(self):\n",
    "        \"\"\"\n",
    "        строит префиксный бор для морфем для более быстрого их поиска\n",
    "        \"\"\"\n",
    "        self.left_morphemes_, self.right_morphemes_ = dict(), dict()\n",
    "        if self.use_morpheme_types:\n",
    "            for key in self.LEFT_MORPHEME_TYPES:\n",
    "                self.left_morphemes_[key] = make_trie(list(self.morphemes_[key]))\n",
    "            for key in self.RIGHT_MORPHEME_TYPES:\n",
    "                self.right_morphemes_[key] = make_trie([x[::-1] for x in self.morphemes_[key]])\n",
    "        if not self.use_morpheme_types or self.to_memorize_ngram_counts:\n",
    "            morphemes = {x for elem in self.morphemes_.values() for x in elem}\n",
    "            self.morpheme_trie_ = make_trie(list(morphemes))\n",
    "        return self\n",
    "\n",
    "    def _predict_probs(self, words, mode_func_appl=True):\n",
    "        \"\"\"\n",
    "        data = [word_1, ..., word_m]\n",
    "\n",
    "        Возвращает:\n",
    "        -------------\n",
    "        answer = [probs_1, ..., probs_m]\n",
    "        probs_i = [p_1, ..., p_k], k = len(word_i)\n",
    "        p_j = [p_j1, ..., p_jr], r --- число классов\n",
    "        (len(AUXILIARY) + 4 * 4 (BMES; PREF, ROOT, SUFF, END) + 3 (BME; POSTFIX) + 2 * 1 (S; LINK, HYPHEN) = 23)\n",
    "        \"\"\"\n",
    "        data_by_buckets, indexes_by_buckets = self._preprocess(words)\n",
    "        word_probs = [None] * len(words)\n",
    "        for r, (bucket_data, (_, bucket_indexes)) in\\\n",
    "                enumerate(zip(data_by_buckets, indexes_by_buckets), 1):\n",
    "            print(\"Bucket {} predicting\".format(r))\n",
    "            bucket_probs = np.mean([model.predict(bucket_data) for model in self.models_], axis=0)\n",
    "            for i, elem in zip(bucket_indexes, bucket_probs):\n",
    "                word_probs[i] = elem\n",
    "        answer = [None] * len(words)\n",
    "        if mode_func_appl:\n",
    "            decode_func = self._decode_best\n",
    "        else:\n",
    "            decode_func = self._decode_as_is\n",
    "        for i, (elem, word) in enumerate(zip(word_probs, words)):\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(\"{} words decoded\".format(i))\n",
    "            answer[i] = decode_func(elem, len(word))\n",
    "        return answer\n",
    "\n",
    "    def labels_to_morphemes(self, word, labels, probs=None, return_probs=False, return_types=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Преобразует ответ из формата BMES в список морфем\n",
    "        Дополнительно может возвращать список вероятностей морфем\n",
    "\n",
    "        word: str, текущее слово,\n",
    "        labels: list of strs, предсказанные метки в формате BMES,\n",
    "        probs: list of floats or None, предсказанные вероятности меток\n",
    "\n",
    "        answer = [morphemes, (morpheme_probs), (morpheme_types)]\n",
    "            morphemes: list of strs, список морфем\n",
    "            morpheme_probs: list of floats, список вероятностей морфем\n",
    "            morpheme_types: list of strs, список типов морфем\n",
    "        \"\"\"\n",
    "        morphemes, curr_morpheme, morpheme_types = [], \"\", []\n",
    "        if self.use_morpheme_types:\n",
    "            end_labels = ['E-ROOT', 'E-PREF', 'E-SUFF', 'E-END', 'E-POSTFIX', 'S-ROOT',\n",
    "                          'S-PREF', 'S-SUFF', 'S-END', 'S-LINK', 'S-HYPH']\n",
    "        else:\n",
    "            end_labels = ['E-None', 'S-None']\n",
    "        for letter, label in zip(word, labels):\n",
    "            curr_morpheme += letter\n",
    "            if label in end_labels:\n",
    "                morphemes.append(curr_morpheme)\n",
    "                curr_morpheme = \"\"\n",
    "                morpheme_types.append(label.split(\"-\")[-1])\n",
    "        if return_probs:\n",
    "            if probs is None:\n",
    "                Warning(\"Для вычисления вероятностей морфем нужно передать вероятности меток\")\n",
    "                return_probs = False\n",
    "        if return_probs:\n",
    "            morpheme_probs, curr_morpheme_prob = [], 1.0\n",
    "            for label, prob in zip(labels, probs):\n",
    "                curr_morpheme_prob *= prob[self.target_symbol_codes_[label]]\n",
    "                if label in end_labels:\n",
    "                    morpheme_probs.append(curr_morpheme_prob)\n",
    "                    curr_morpheme_prob = 1.0\n",
    "            answer = [morphemes, morpheme_probs]\n",
    "        else:\n",
    "            answer = [morphemes]\n",
    "        if return_types:\n",
    "            answer.append(morpheme_types)\n",
    "        return answer\n",
    "\n",
    "    def predict(self, words, return_probs=False):\n",
    "        labels_with_probs = self._predict_probs(words)\n",
    "        return [self.labels_to_morphemes(word, elem[0], elem[1], return_probs=return_probs)\n",
    "                for elem, word in zip(labels_with_probs, words)]\n",
    "\n",
    "    def _decode_best(self, probs, length):\n",
    "        \"\"\"\n",
    "        Поддерживаем в каждой позиции наилучшие гипотезы для каждого состояния\n",
    "        Состояние --- последняя предсказанняя метка\n",
    "        \"\"\"\n",
    "        # вначале нужно проверить заведомо наилучший вариант на корректность\n",
    "        best_states = np.argmax(probs[:1+length], axis=1)\n",
    "        best_labels = [self.target_symbols_[state_index] for state_index in best_states]\n",
    "        if not is_correct_morpheme_sequence(best_labels[1:]):\n",
    "            # наилучший вариант оказался некорректным\n",
    "            initial_costs = [np.inf] * self.target_symbols_number_\n",
    "            initial_states = [None] * self.target_symbols_number_\n",
    "            initial_costs[BEGIN], initial_states[BEGIN] = -np.log(probs[0, BEGIN]), BEGIN\n",
    "            costs, states = [initial_costs], [initial_states]\n",
    "            for i in range(length):\n",
    "                # состояний мало, поэтому можно сортировать на каждом шаге\n",
    "                state_order = np.argsort(costs[-1])\n",
    "                curr_costs = [np.inf] * self.target_symbols_number_\n",
    "                prev_states = [None] * self.target_symbols_number_\n",
    "                inf_count = self.target_symbols_number_\n",
    "                for prev_state in state_order:\n",
    "                    if np.isinf(costs[-1][prev_state]):\n",
    "                        break\n",
    "                    elif prev_state in AUXILIARY_CODES and i != 0:\n",
    "                        continue\n",
    "                    possible_states = self.get_possible_next_states(prev_state)\n",
    "                    for state in possible_states:\n",
    "                        if np.isinf(curr_costs[state]):\n",
    "                            # поскольку новая вероятность не зависит от state,\n",
    "                            # а старые перебираются по возрастанию штрафа,\n",
    "                            # то оптимальное значение будет при первом обновлении\n",
    "                            curr_costs[state] = costs[-1][prev_state] - np.log(probs[i+1,state])\n",
    "                            prev_states[state] = prev_state\n",
    "                            inf_count -= 1\n",
    "                    if inf_count == len(AUXILIARY_CODES):\n",
    "                        # все вероятности уже посчитаны\n",
    "                        break\n",
    "                costs.append(curr_costs)\n",
    "                states.append(prev_states)\n",
    "            # последнее состояние --- обязательно конец морфемы\n",
    "            possible_states = [self.target_symbol_codes_[\"{}-{}\".format(x, y)]\n",
    "                               for x in \"ES\" for y in [\"ROOT\", \"SUFF\", \"END\", \"POSTFIX\", \"None\"]\n",
    "                               if \"{}-{}\".format(x, y) in self.target_symbol_codes_]\n",
    "            best_states = [min(possible_states, key=(lambda x: costs[-1][x]))]\n",
    "            for j in range(length, 0, -1):\n",
    "                # предыдущее состояние\n",
    "                best_states.append(states[j][best_states[-1]])\n",
    "            best_states = best_states[::-1]\n",
    "        probs_to_return = np.zeros(shape=(length, self.target_symbols_number_), dtype=np.float32)\n",
    "        # убираем невозможные состояния\n",
    "        for j, state in enumerate(best_states[:-1]):\n",
    "            possible_states = self.get_possible_next_states(state)\n",
    "            # оставляем только возможные состояния.\n",
    "            probs_to_return[j,possible_states] = probs[j+1,possible_states]\n",
    "        return [self.target_symbols_[i] for i in best_states[1:]], probs_to_return\n",
    "    \n",
    "    def _decode_as_is(self, probs, length):\n",
    "        probs_to_return = probs[1:1+length]\n",
    "        best_states = np.argmax(probs_to_return, axis=1)\n",
    "        best_labels = [self.target_symbols_[state_index] for state_index in best_states]\n",
    "        return best_labels, probs_to_return\n",
    "        \n",
    "\n",
    "    def get_possible_next_states(self, state_index):\n",
    "        state = self.target_symbols_[state_index]\n",
    "        next_states = get_next_morpheme(state)\n",
    "        return [self.target_symbol_codes_[x] for x in next_states if x in self.target_symbol_codes_]\n",
    "\n",
    "\n",
    "def generate_data(data, targets, indexes, classes_number, shuffle=False, nepochs=None):\n",
    "    \"\"\"\n",
    "\n",
    "    data: list of lists of arrays,\n",
    "        data = [bucket_1, ..., bucket_m],\n",
    "        bucket = [input_1, ..., input_k], k --- число входов в графе вычислений\n",
    "    targets: list of arrays,\n",
    "        targets[i,j] --- код j-ой метки при морфемоделении i-го слова\n",
    "    indexes: list of pairs,\n",
    "        indexes = [(i_1, bucket_indexes_1), ...]\n",
    "        i_j --- номер корзины, откуда берутся элементы j-го батча\n",
    "        bucket_indexes_j --- номера элементов j-го батча в соответствующей корзине\n",
    "    shuffle: boolean, default=False, нужно ли перемешивать порядок батчей\n",
    "    nepochs: int or None, default=None,\n",
    "        число эпох, в течение которых генератор выдаёт батчи, в случае None генератор бесконечен\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    nsteps = 0\n",
    "    while nepochs is None or nsteps < nepochs:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        for i, bucket_indexes in indexes:\n",
    "            curr_bucket, curr_targets = data[i], targets[i]\n",
    "            data_to_yield = [elem[bucket_indexes] for elem in curr_bucket]\n",
    "            targets_to_yield = to_one_hot(curr_targets[bucket_indexes], classes_number)\n",
    "            yield data_to_yield, targets_to_yield\n",
    "        nsteps += 1\n",
    "\n",
    "\n",
    "def measure_quality(targets, predicted_targets, english_metrics=False, measure_last=True):\n",
    "    \"\"\"\n",
    "\n",
    "    targets: метки корректных ответов\n",
    "    predicted_targets: метки предсказанных ответов\n",
    "\n",
    "    Возвращает словарь со значениями основных метрик\n",
    "    \"\"\"\n",
    "    TP, FP, FN, equal, total = 0, 0, 0, 0, 0\n",
    "    SE = ['{}-{}'.format(x, y) for x in \"SE\" for y in [\"ROOT\", \"PREF\", \"SUFF\", \"END\", \"LINK\", \"None\"]]\n",
    "    # SE = ['S-ROOT', 'S-PREF', 'S-SUFF', 'S-END', 'S-LINK', 'E-ROOT', 'E-PREF', 'E-SUFF', 'E-END']\n",
    "    corr_words = 0\n",
    "    for corr, pred in zip(targets, predicted_targets):\n",
    "        corr_len = len(corr) + int(measure_last) - 1\n",
    "        pred_len = len(pred) + int(measure_last) - 1\n",
    "        boundaries = [i for i in range(corr_len) if corr[i] in SE]\n",
    "        pred_boundaries = [i for i in range(pred_len) if pred[i] in SE]\n",
    "        common = [x for x in boundaries if x in pred_boundaries]\n",
    "        TP += len(common)\n",
    "        FN += len(boundaries) - len(common)\n",
    "        FP += len(pred_boundaries) - len(common)\n",
    "        equal += sum(int(x==y) for x, y in zip(corr, pred))\n",
    "        total += len(corr)\n",
    "        corr_words += (corr == pred)\n",
    "    metrics = [\"Точность\", \"Полнота\", \"F1-мера\", \"Корректность\", \"Точность по словам\"]\n",
    "    if english_metrics:\n",
    "        metrics = [\"Precision\", \"Recall\", \"F1\", \"Accuracy\", \"Word accuracy\"]\n",
    "    results = [TP / (TP+FP), TP / (TP+FN), TP / (TP + 0.5*(FP+FN)),\n",
    "               equal / total, corr_words / len(targets)]\n",
    "    answer = list(zip(metrics, results))\n",
    "    return answer\n",
    "\n",
    "\n",
    "SHORT_ARGS = \"a:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запуски моделей в разных режимах, тестирование\n",
    "В дальнейшем коде были произведены как запуски предобученной модели, так и обучение модели на собственной разметке и сравнение результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Директория с кодом и вспомогателными фалами конфига, датасетами и прочего\n",
    "cur_dir = \"/Users/a0154158/Desktop/Дела-делишки_2/мага 2 курс/дисер/NeuralMorphemeSegmentation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = read_config(cur_dir+\"config/morph_config_3-5-use.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже происходит загрузка предобученной модели из оригинального git используемой исследовательской работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'conv_layers': 3, 'window_size': [5], 'filters_number': 192, 'dense_output_units': 64, 'nepochs': 75, 'validation_split': 0.2, 'early_stopping': 10, 'to_memorize_morphemes': False, 'dropout': 0.2, 'to_memorize_ngram_counts': False, 'context_dropout': 0.2, 'models_number': 1}, 'save_file': 'models/morphemes-3-5-lpnsk.json', 'model_file': 'models/morphemes-model-3-5-lpnsk.hdf5', 'test_file': 'data/test_Tikhonov_reformat.txt', 'outfile': 'results/test_3-5.txt', 'load_file': 'models/morphemes-3-5-3_lpnsk.json', 'use_morpheme_types': True}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "symbol_inputs (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "symbol_embeddings (Lambda)   (None, None, 37)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 192)         35712     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 192)         184512    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, None, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 192)         184512    \n",
      "_________________________________________________________________\n",
      "pre_output (TimeDistributed) (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, None, 25)          1625      \n",
      "=================================================================\n",
      "Total params: 418,713\n",
      "Trainable params: 418,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "use_morpheme_types = params[\"use_morpheme_types\"]\n",
    "print(params)\n",
    "read_func = read_BMES\n",
    "inputs, targets, dev_inputs, dev_targets = None, None, None, None\n",
    "cls = load_cls(cur_dir+params[\"load_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = read_func(params[\"test_file\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск модели на тестовой выборке (загружена в переменную inputs) ранее. predicted_targets -- запуск модели с использованием улучшающего алгоритма, predicted_targets_raw -- без него. Оценка производительности будет представлена ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1 predicting\n",
      "Bucket 2 predicting\n",
      "Bucket 3 predicting\n",
      "Bucket 4 predicting\n",
      "Bucket 5 predicting\n",
      "Bucket 6 predicting\n",
      "Bucket 7 predicting\n",
      "Bucket 8 predicting\n",
      "Bucket 9 predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:789: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 words decoded\n",
      "2000 words decoded\n",
      "3000 words decoded\n",
      "4000 words decoded\n",
      "5000 words decoded\n",
      "6000 words decoded\n",
      "7000 words decoded\n",
      "8000 words decoded\n",
      "9000 words decoded\n",
      "10000 words decoded\n",
      "11000 words decoded\n",
      "12000 words decoded\n",
      "13000 words decoded\n",
      "14000 words decoded\n",
      "15000 words decoded\n",
      "16000 words decoded\n",
      "17000 words decoded\n",
      "18000 words decoded\n",
      "19000 words decoded\n",
      "20000 words decoded\n",
      "21000 words decoded\n",
      "22000 words decoded\n",
      "23000 words decoded\n",
      "24000 words decoded\n"
     ]
    }
   ],
   "source": [
    "predicted_targets = cls._predict_probs(inputs)\n",
    "#measure_last = params.get(\"measure_last\", use_morpheme_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1 predicting\n",
      "Bucket 2 predicting\n",
      "Bucket 3 predicting\n",
      "Bucket 4 predicting\n",
      "Bucket 5 predicting\n",
      "Bucket 6 predicting\n",
      "Bucket 7 predicting\n",
      "Bucket 8 predicting\n",
      "Bucket 9 predicting\n",
      "1000 words decoded\n",
      "2000 words decoded\n",
      "3000 words decoded\n",
      "4000 words decoded\n",
      "5000 words decoded\n",
      "6000 words decoded\n",
      "7000 words decoded\n",
      "8000 words decoded\n",
      "9000 words decoded\n",
      "10000 words decoded\n",
      "11000 words decoded\n",
      "12000 words decoded\n",
      "13000 words decoded\n",
      "14000 words decoded\n",
      "15000 words decoded\n",
      "16000 words decoded\n",
      "17000 words decoded\n",
      "18000 words decoded\n",
      "19000 words decoded\n",
      "20000 words decoded\n",
      "21000 words decoded\n",
      "22000 words decoded\n",
      "23000 words decoded\n",
      "24000 words decoded\n"
     ]
    }
   ],
   "source": [
    "predicted_targets_raw = cls._predict_probs(inputs, mode_func_appl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск обучения модели с использованием сокращённой разметки BMES. В переменных inputs_tr, targets_tr данные из части словаря Тихонова, используемого для обучения (train_Tikhono_reformat.txt). Как и в показанном ранее примере, функции read_BMES передаётся параметром функций BMES_func = generate_BMES_shrt для осуществления разметки нового типа. cls_new - экземпляр модели, обученный на новой разметке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(261)\n",
    "params_for_learn = read_config(cur_dir+\"config/morph_config_3-5-train.json\")\n",
    "dev_inputs, dev_targets = None, None\n",
    "n = params_for_learn.get(\"n_train\") # число слов в обучающей+развивающей выборке\n",
    "#inputs2, targets2 = read_BMES(cur_dir+params[\"test_file\"], shuffle=False, BMES_func = generate_BMES_shrt)\n",
    "use_morpheme_types = params_for_learn[\"use_morpheme_types\"]\n",
    "inputs_tr, targets_tr = read_BMES(cur_dir+\"data/train_Tikhonov_reformat.txt\", shuffle=False, BMES_func = generate_BMES_shrt, n=n)\n",
    "partitioner_params = params_for_learn.get(\"model_params\", dict())\n",
    "partitioner_params[\"use_morpheme_types\"] = use_morpheme_types\n",
    "cls_new = Partitioner(**partitioner_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "symbol_inputs (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "symbol_embeddings (Lambda)   (None, None, 37)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, None, 192)         35712     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, None, 192)         184512    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, None, 192)         184512    \n",
      "_________________________________________________________________\n",
      "pre_output (TimeDistributed) (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, None, 19)          1235      \n",
      "=================================================================\n",
      "Total params: 418,323\n",
      "Trainable params: 418,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/75\n",
      "1804/1804 [==============================] - 170s 94ms/step - loss: 0.3353 - acc: 0.8846 - val_loss: 0.2092 - val_acc: 0.9249\n",
      "Epoch 2/75\n",
      "1804/1804 [==============================] - 161s 89ms/step - loss: 0.2010 - acc: 0.9280 - val_loss: 0.1735 - val_acc: 0.9371\n",
      "Epoch 3/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.1718 - acc: 0.9383 - val_loss: 0.1522 - val_acc: 0.9447\n",
      "Epoch 4/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.1526 - acc: 0.9451 - val_loss: 0.1448 - val_acc: 0.9488\n",
      "Epoch 5/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.1390 - acc: 0.9499 - val_loss: 0.1350 - val_acc: 0.9524\n",
      "Epoch 6/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.1283 - acc: 0.9539 - val_loss: 0.1288 - val_acc: 0.9541\n",
      "Epoch 7/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.1197 - acc: 0.9566 - val_loss: 0.1223 - val_acc: 0.9568\n",
      "Epoch 8/75\n",
      "1804/1804 [==============================] - 156s 87ms/step - loss: 0.1117 - acc: 0.9596 - val_loss: 0.1174 - val_acc: 0.9589\n",
      "Epoch 9/75\n",
      "1804/1804 [==============================] - 157s 87ms/step - loss: 0.1051 - acc: 0.9617 - val_loss: 0.1172 - val_acc: 0.9591\n",
      "Epoch 10/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.1004 - acc: 0.9636 - val_loss: 0.1157 - val_acc: 0.9605\n",
      "Epoch 11/75\n",
      "1804/1804 [==============================] - 159s 88ms/step - loss: 0.0962 - acc: 0.9650 - val_loss: 0.1126 - val_acc: 0.9608\n",
      "Epoch 12/75\n",
      "1804/1804 [==============================] - 157s 87ms/step - loss: 0.0913 - acc: 0.9666 - val_loss: 0.1096 - val_acc: 0.9631\n",
      "Epoch 13/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.0885 - acc: 0.9675 - val_loss: 0.1089 - val_acc: 0.9631\n",
      "Epoch 14/75\n",
      "1804/1804 [==============================] - 156s 87ms/step - loss: 0.0846 - acc: 0.9690 - val_loss: 0.1074 - val_acc: 0.9640\n",
      "Epoch 15/75\n",
      "1804/1804 [==============================] - 153s 85ms/step - loss: 0.0834 - acc: 0.9698 - val_loss: 0.1062 - val_acc: 0.9641\n",
      "Epoch 16/75\n",
      "1804/1804 [==============================] - 156s 86ms/step - loss: 0.0796 - acc: 0.9712 - val_loss: 0.1050 - val_acc: 0.9649\n",
      "Epoch 17/75\n",
      "1804/1804 [==============================] - 154s 85ms/step - loss: 0.0773 - acc: 0.9715 - val_loss: 0.1050 - val_acc: 0.9656\n",
      "Epoch 18/75\n",
      "1804/1804 [==============================] - 154s 86ms/step - loss: 0.0746 - acc: 0.9729 - val_loss: 0.1055 - val_acc: 0.9662\n",
      "Epoch 19/75\n",
      "1804/1804 [==============================] - 156s 87ms/step - loss: 0.0733 - acc: 0.9733 - val_loss: 0.1053 - val_acc: 0.9655\n",
      "Epoch 20/75\n",
      "1804/1804 [==============================] - 153s 85ms/step - loss: 0.0719 - acc: 0.9737 - val_loss: 0.1057 - val_acc: 0.9664\n",
      "Epoch 21/75\n",
      "1804/1804 [==============================] - 158s 87ms/step - loss: 0.0696 - acc: 0.9746 - val_loss: 0.1080 - val_acc: 0.9654\n",
      "Epoch 22/75\n",
      "1804/1804 [==============================] - 157s 87ms/step - loss: 0.0682 - acc: 0.9754 - val_loss: 0.1037 - val_acc: 0.9663\n",
      "Epoch 23/75\n",
      "1804/1804 [==============================] - 156s 86ms/step - loss: 0.0671 - acc: 0.9757 - val_loss: 0.1051 - val_acc: 0.9664\n",
      "Epoch 24/75\n",
      "1804/1804 [==============================] - 158s 88ms/step - loss: 0.0661 - acc: 0.9761 - val_loss: 0.1051 - val_acc: 0.9673\n",
      "Epoch 25/75\n",
      "1804/1804 [==============================] - 158s 87ms/step - loss: 0.0644 - acc: 0.9766 - val_loss: 0.1064 - val_acc: 0.9671\n",
      "Epoch 26/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0636 - acc: 0.9769 - val_loss: 0.1046 - val_acc: 0.9674\n",
      "Epoch 27/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0622 - acc: 0.9772 - val_loss: 0.1051 - val_acc: 0.9681\n",
      "Epoch 28/75\n",
      "1804/1804 [==============================] - 142s 79ms/step - loss: 0.0606 - acc: 0.9780 - val_loss: 0.1043 - val_acc: 0.9678\n",
      "Epoch 29/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0598 - acc: 0.9783 - val_loss: 0.1022 - val_acc: 0.9676\n",
      "Epoch 30/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0590 - acc: 0.9784 - val_loss: 0.1034 - val_acc: 0.9677\n",
      "Epoch 31/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0580 - acc: 0.9790 - val_loss: 0.1032 - val_acc: 0.9678\n",
      "Epoch 32/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0571 - acc: 0.9794 - val_loss: 0.1044 - val_acc: 0.9679\n",
      "Epoch 33/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0559 - acc: 0.9796 - val_loss: 0.1032 - val_acc: 0.9685\n",
      "Epoch 34/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0560 - acc: 0.9796 - val_loss: 0.1025 - val_acc: 0.9687\n",
      "Epoch 35/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0550 - acc: 0.9801 - val_loss: 0.1030 - val_acc: 0.9683\n",
      "Epoch 36/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0547 - acc: 0.9801 - val_loss: 0.1061 - val_acc: 0.9685\n",
      "Epoch 37/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0533 - acc: 0.9807 - val_loss: 0.1056 - val_acc: 0.9684\n",
      "Epoch 38/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0530 - acc: 0.9808 - val_loss: 0.1094 - val_acc: 0.9682\n",
      "Epoch 39/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0514 - acc: 0.9814 - val_loss: 0.1054 - val_acc: 0.9687\n",
      "Epoch 40/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0520 - acc: 0.9812 - val_loss: 0.1055 - val_acc: 0.9691\n",
      "Epoch 41/75\n",
      "1804/1804 [==============================] - 145s 81ms/step - loss: 0.0508 - acc: 0.9816 - val_loss: 0.1036 - val_acc: 0.9696\n",
      "Epoch 42/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0506 - acc: 0.9818 - val_loss: 0.1033 - val_acc: 0.9696\n",
      "Epoch 43/75\n",
      "1804/1804 [==============================] - 145s 81ms/step - loss: 0.0506 - acc: 0.9817 - val_loss: 0.1045 - val_acc: 0.9692\n",
      "Epoch 44/75\n",
      "1804/1804 [==============================] - 143s 80ms/step - loss: 0.0496 - acc: 0.9821 - val_loss: 0.1050 - val_acc: 0.9691\n",
      "Epoch 45/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0499 - acc: 0.9820 - val_loss: 0.1055 - val_acc: 0.9694\n",
      "Epoch 46/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0484 - acc: 0.9824 - val_loss: 0.1080 - val_acc: 0.9687\n",
      "Epoch 47/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0485 - acc: 0.9824 - val_loss: 0.1080 - val_acc: 0.9687\n",
      "Epoch 48/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0479 - acc: 0.9826 - val_loss: 0.1071 - val_acc: 0.9689\n",
      "Epoch 49/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0469 - acc: 0.9831 - val_loss: 0.1077 - val_acc: 0.9694\n",
      "Epoch 50/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0473 - acc: 0.9828 - val_loss: 0.1058 - val_acc: 0.9692\n",
      "Epoch 51/75\n",
      "1804/1804 [==============================] - 143s 79ms/step - loss: 0.0459 - acc: 0.9833 - val_loss: 0.1061 - val_acc: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Partitioner at 0xb38362e48>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_new.train(inputs_tr, targets_tr, dev_inputs, dev_targets, model_file=params_for_learn.get(\"model_file\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1 predicting\n",
      "Bucket 2 predicting\n",
      "Bucket 3 predicting\n",
      "Bucket 4 predicting\n",
      "Bucket 5 predicting\n",
      "Bucket 6 predicting\n",
      "Bucket 7 predicting\n",
      "Bucket 8 predicting\n",
      "Bucket 9 predicting\n",
      "1000 words decoded\n",
      "2000 words decoded\n",
      "3000 words decoded\n",
      "4000 words decoded\n",
      "5000 words decoded\n",
      "6000 words decoded\n",
      "7000 words decoded\n",
      "8000 words decoded\n",
      "9000 words decoded\n",
      "10000 words decoded\n",
      "11000 words decoded\n",
      "12000 words decoded\n",
      "13000 words decoded\n",
      "14000 words decoded\n",
      "15000 words decoded\n",
      "16000 words decoded\n",
      "17000 words decoded\n",
      "18000 words decoded\n",
      "19000 words decoded\n",
      "20000 words decoded\n",
      "21000 words decoded\n",
      "22000 words decoded\n",
      "23000 words decoded\n",
      "24000 words decoded\n"
     ]
    }
   ],
   "source": [
    "pred_targets_new_model_raw = cls_new._predict_probs(inputs, mode_func_appl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск обучения изначальной версии модели. В переменной targets2_tr тренировочная часть датасета, размеченная обычным способом. cls_old - экземпляр модели, заново обученный на данных с обычной BMES-разметкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_for_learn = read_config(cur_dir+\"config/morph_config_3-5-train.json\")\n",
    "np.random.seed(261)\n",
    "dev_inputs, dev_targets = None, None\n",
    "n = params_for_learn.get(\"n_train\") # число слов в обучающей+развивающей выборке\n",
    "#inputs2, targets2 = read_BMES(cur_dir+params[\"test_file\"], shuffle=False, BMES_func = generate_BMES_shrt)\n",
    "use_morpheme_types = params_for_learn[\"use_morpheme_types\"]\n",
    "_, targets2_tr = read_BMES(cur_dir+\"data/train_Tikhonov_reformat.txt\", shuffle=False, BMES_func = generate_BMES, n=n)\n",
    "partitioner_params = params_for_learn.get(\"model_params\", dict())\n",
    "partitioner_params[\"use_morpheme_types\"] = use_morpheme_types\n",
    "cls_old = Partitioner(**partitioner_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "symbol_inputs (InputLayer)   (None, None)              0         \n",
      "_________________________________________________________________\n",
      "symbol_embeddings (Lambda)   (None, None, 37)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, None, 192)         35712     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, None, 192)         184512    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, None, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, None, 192)         184512    \n",
      "_________________________________________________________________\n",
      "pre_output (TimeDistributed) (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, None, 25)          1625      \n",
      "=================================================================\n",
      "Total params: 418,713\n",
      "Trainable params: 418,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/75\n",
      "1804/1804 [==============================] - 148s 82ms/step - loss: 0.3975 - acc: 0.8657 - val_loss: 0.2457 - val_acc: 0.9131\n",
      "Epoch 2/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.2396 - acc: 0.9150 - val_loss: 0.2054 - val_acc: 0.9272\n",
      "Epoch 3/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.2041 - acc: 0.9276 - val_loss: 0.1810 - val_acc: 0.9361\n",
      "Epoch 4/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.1800 - acc: 0.9361 - val_loss: 0.1699 - val_acc: 0.9417\n",
      "Epoch 5/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.1637 - acc: 0.9417 - val_loss: 0.1583 - val_acc: 0.9450\n",
      "Epoch 6/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.1511 - acc: 0.9462 - val_loss: 0.1477 - val_acc: 0.9485\n",
      "Epoch 7/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.1414 - acc: 0.9493 - val_loss: 0.1445 - val_acc: 0.9503\n",
      "Epoch 8/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.1320 - acc: 0.9524 - val_loss: 0.1393 - val_acc: 0.9527\n",
      "Epoch 9/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.1246 - acc: 0.9551 - val_loss: 0.1325 - val_acc: 0.9544\n",
      "Epoch 10/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.1172 - acc: 0.9578 - val_loss: 0.1350 - val_acc: 0.9542\n",
      "Epoch 11/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.1116 - acc: 0.9601 - val_loss: 0.1312 - val_acc: 0.9549\n",
      "Epoch 12/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.1081 - acc: 0.9610 - val_loss: 0.1281 - val_acc: 0.9566\n",
      "Epoch 13/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.1037 - acc: 0.9626 - val_loss: 0.1266 - val_acc: 0.9573\n",
      "Epoch 14/75\n",
      "1804/1804 [==============================] - 151s 84ms/step - loss: 0.1004 - acc: 0.9636 - val_loss: 0.1237 - val_acc: 0.9582\n",
      "Epoch 15/75\n",
      "1804/1804 [==============================] - 149s 83ms/step - loss: 0.0976 - acc: 0.9647 - val_loss: 0.1228 - val_acc: 0.9586\n",
      "Epoch 16/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0950 - acc: 0.9653 - val_loss: 0.1239 - val_acc: 0.9590\n",
      "Epoch 17/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0920 - acc: 0.9670 - val_loss: 0.1221 - val_acc: 0.9595\n",
      "Epoch 18/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.0892 - acc: 0.9678 - val_loss: 0.1240 - val_acc: 0.9593\n",
      "Epoch 19/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0873 - acc: 0.9687 - val_loss: 0.1226 - val_acc: 0.9597\n",
      "Epoch 20/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.0860 - acc: 0.9691 - val_loss: 0.1201 - val_acc: 0.9602\n",
      "Epoch 21/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.0829 - acc: 0.9700 - val_loss: 0.1201 - val_acc: 0.9614\n",
      "Epoch 22/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0818 - acc: 0.9705 - val_loss: 0.1206 - val_acc: 0.9605\n",
      "Epoch 23/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0798 - acc: 0.9711 - val_loss: 0.1194 - val_acc: 0.9613\n",
      "Epoch 24/75\n",
      "1804/1804 [==============================] - 147s 82ms/step - loss: 0.0798 - acc: 0.9712 - val_loss: 0.1211 - val_acc: 0.9617\n",
      "Epoch 25/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0770 - acc: 0.9722 - val_loss: 0.1208 - val_acc: 0.9608\n",
      "Epoch 26/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0755 - acc: 0.9729 - val_loss: 0.1195 - val_acc: 0.9623\n",
      "Epoch 27/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0745 - acc: 0.9730 - val_loss: 0.1197 - val_acc: 0.9630\n",
      "Epoch 28/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0730 - acc: 0.9738 - val_loss: 0.1211 - val_acc: 0.9615\n",
      "Epoch 29/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0711 - acc: 0.9743 - val_loss: 0.1178 - val_acc: 0.9632\n",
      "Epoch 30/75\n",
      "1804/1804 [==============================] - 145s 81ms/step - loss: 0.0712 - acc: 0.9744 - val_loss: 0.1181 - val_acc: 0.9627\n",
      "Epoch 31/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0696 - acc: 0.9750 - val_loss: 0.1197 - val_acc: 0.9625\n",
      "Epoch 32/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0690 - acc: 0.9751 - val_loss: 0.1201 - val_acc: 0.9622\n",
      "Epoch 33/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0694 - acc: 0.9749 - val_loss: 0.1192 - val_acc: 0.9633\n",
      "Epoch 34/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0676 - acc: 0.9755 - val_loss: 0.1193 - val_acc: 0.9635\n",
      "Epoch 35/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0672 - acc: 0.9760 - val_loss: 0.1203 - val_acc: 0.9633\n",
      "Epoch 36/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0653 - acc: 0.9763 - val_loss: 0.1222 - val_acc: 0.9638\n",
      "Epoch 37/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0654 - acc: 0.9766 - val_loss: 0.1219 - val_acc: 0.9631\n",
      "Epoch 38/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0649 - acc: 0.9766 - val_loss: 0.1189 - val_acc: 0.9641\n",
      "Epoch 39/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.0630 - acc: 0.9772 - val_loss: 0.1209 - val_acc: 0.9642\n",
      "Epoch 40/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0622 - acc: 0.9777 - val_loss: 0.1210 - val_acc: 0.9646\n",
      "Epoch 41/75\n",
      "1804/1804 [==============================] - 144s 80ms/step - loss: 0.0609 - acc: 0.9781 - val_loss: 0.1205 - val_acc: 0.9645\n",
      "Epoch 42/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0613 - acc: 0.9778 - val_loss: 0.1204 - val_acc: 0.9645\n",
      "Epoch 43/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0612 - acc: 0.9780 - val_loss: 0.1189 - val_acc: 0.9643\n",
      "Epoch 44/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0613 - acc: 0.9780 - val_loss: 0.1197 - val_acc: 0.9645\n",
      "Epoch 45/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0590 - acc: 0.9788 - val_loss: 0.1194 - val_acc: 0.9649\n",
      "Epoch 46/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0593 - acc: 0.9786 - val_loss: 0.1202 - val_acc: 0.9646\n",
      "Epoch 47/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0581 - acc: 0.9789 - val_loss: 0.1190 - val_acc: 0.9648\n",
      "Epoch 48/75\n",
      "1804/1804 [==============================] - 145s 81ms/step - loss: 0.0587 - acc: 0.9790 - val_loss: 0.1190 - val_acc: 0.9650\n",
      "Epoch 49/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0578 - acc: 0.9791 - val_loss: 0.1235 - val_acc: 0.9638\n",
      "Epoch 50/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0567 - acc: 0.9794 - val_loss: 0.1226 - val_acc: 0.9648\n",
      "Epoch 51/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0570 - acc: 0.9796 - val_loss: 0.1185 - val_acc: 0.9650\n",
      "Epoch 52/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0557 - acc: 0.9798 - val_loss: 0.1238 - val_acc: 0.9651\n",
      "Epoch 53/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0559 - acc: 0.9800 - val_loss: 0.1189 - val_acc: 0.9648\n",
      "Epoch 54/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0557 - acc: 0.9801 - val_loss: 0.1217 - val_acc: 0.9654\n",
      "Epoch 55/75\n",
      "1804/1804 [==============================] - 145s 80ms/step - loss: 0.0546 - acc: 0.9806 - val_loss: 0.1235 - val_acc: 0.9656\n",
      "Epoch 56/75\n",
      "1804/1804 [==============================] - 145s 81ms/step - loss: 0.0545 - acc: 0.9804 - val_loss: 0.1233 - val_acc: 0.9650\n",
      "Epoch 57/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0542 - acc: 0.9806 - val_loss: 0.1233 - val_acc: 0.9656\n",
      "Epoch 58/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0538 - acc: 0.9807 - val_loss: 0.1232 - val_acc: 0.9655\n",
      "Epoch 59/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0533 - acc: 0.9808 - val_loss: 0.1223 - val_acc: 0.9654\n",
      "Epoch 60/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0528 - acc: 0.9810 - val_loss: 0.1236 - val_acc: 0.9653\n",
      "Epoch 61/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0526 - acc: 0.9812 - val_loss: 0.1251 - val_acc: 0.9654\n",
      "Epoch 62/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0529 - acc: 0.9812 - val_loss: 0.1218 - val_acc: 0.9663\n",
      "Epoch 63/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0525 - acc: 0.9814 - val_loss: 0.1205 - val_acc: 0.9659\n",
      "Epoch 64/75\n",
      "1804/1804 [==============================] - 149s 83ms/step - loss: 0.0507 - acc: 0.9819 - val_loss: 0.1244 - val_acc: 0.9658\n",
      "Epoch 65/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0504 - acc: 0.9818 - val_loss: 0.1249 - val_acc: 0.9648\n",
      "Epoch 66/75\n",
      "1804/1804 [==============================] - 148s 82ms/step - loss: 0.0518 - acc: 0.9815 - val_loss: 0.1213 - val_acc: 0.9659\n",
      "Epoch 67/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0501 - acc: 0.9821 - val_loss: 0.1240 - val_acc: 0.9653\n",
      "Epoch 68/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0499 - acc: 0.9822 - val_loss: 0.1261 - val_acc: 0.9654\n",
      "Epoch 69/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0499 - acc: 0.9821 - val_loss: 0.1228 - val_acc: 0.9655\n",
      "Epoch 70/75\n",
      "1804/1804 [==============================] - 147s 82ms/step - loss: 0.0497 - acc: 0.9823 - val_loss: 0.1225 - val_acc: 0.9658\n",
      "Epoch 71/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0493 - acc: 0.9823 - val_loss: 0.1202 - val_acc: 0.9665\n",
      "Epoch 72/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0483 - acc: 0.9827 - val_loss: 0.1249 - val_acc: 0.9656\n",
      "Epoch 73/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.0488 - acc: 0.9826 - val_loss: 0.1241 - val_acc: 0.9663\n",
      "Epoch 74/75\n",
      "1804/1804 [==============================] - 146s 81ms/step - loss: 0.0486 - acc: 0.9828 - val_loss: 0.1268 - val_acc: 0.9661\n",
      "Epoch 75/75\n",
      "1804/1804 [==============================] - 147s 81ms/step - loss: 0.0487 - acc: 0.9825 - val_loss: 0.1213 - val_acc: 0.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Partitioner at 0xb3c7711d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_old.train(inputs_tr, targets2_tr, dev_inputs, dev_targets, model_file=params_for_learn.get(\"model_file\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели в действии: морфемный разбор тестовой части словаря Тихонова. predicted_targets_new - классификация на сокращённое количество классов BMES, predicted_targets_old - классификация на обычное количество классов BMES. Стоит отметить, что обе используются без дополнительного алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1 predicting\n",
      "Bucket 2 predicting\n",
      "Bucket 3 predicting\n",
      "Bucket 4 predicting\n",
      "Bucket 5 predicting\n",
      "Bucket 6 predicting\n",
      "Bucket 7 predicting\n",
      "Bucket 8 predicting\n",
      "Bucket 9 predicting\n",
      "1000 words decoded\n",
      "2000 words decoded\n",
      "3000 words decoded\n",
      "4000 words decoded\n",
      "5000 words decoded\n",
      "6000 words decoded\n",
      "7000 words decoded\n",
      "8000 words decoded\n",
      "9000 words decoded\n",
      "10000 words decoded\n",
      "11000 words decoded\n",
      "12000 words decoded\n",
      "13000 words decoded\n",
      "14000 words decoded\n",
      "15000 words decoded\n",
      "16000 words decoded\n",
      "17000 words decoded\n",
      "18000 words decoded\n",
      "19000 words decoded\n",
      "20000 words decoded\n",
      "21000 words decoded\n",
      "22000 words decoded\n",
      "23000 words decoded\n",
      "24000 words decoded\n"
     ]
    }
   ],
   "source": [
    "predicted_targets_new = cls_new._predict_probs(inputs, mode_func_appl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1 predicting\n",
      "Bucket 2 predicting\n",
      "Bucket 3 predicting\n",
      "Bucket 4 predicting\n",
      "Bucket 5 predicting\n",
      "Bucket 6 predicting\n",
      "Bucket 7 predicting\n",
      "Bucket 8 predicting\n",
      "Bucket 9 predicting\n",
      "1000 words decoded\n",
      "2000 words decoded\n",
      "3000 words decoded\n",
      "4000 words decoded\n",
      "5000 words decoded\n",
      "6000 words decoded\n",
      "7000 words decoded\n",
      "8000 words decoded\n",
      "9000 words decoded\n",
      "10000 words decoded\n",
      "11000 words decoded\n",
      "12000 words decoded\n",
      "13000 words decoded\n",
      "14000 words decoded\n",
      "15000 words decoded\n",
      "16000 words decoded\n",
      "17000 words decoded\n",
      "18000 words decoded\n",
      "19000 words decoded\n",
      "20000 words decoded\n",
      "21000 words decoded\n",
      "22000 words decoded\n",
      "23000 words decoded\n",
      "24000 words decoded\n"
     ]
    }
   ],
   "source": [
    "predicted_targets_old = cls_old._predict_probs(inputs, mode_func_appl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение предикта двух моделей: видно, что первая модель осуществляет классификацию на меньшее количество классов, нежели вторая. На данном примере обе модели справились с задачей сегментации верно. Из кода также видно, что в первом варианте в модели действительно меньшее количество классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "воскрыляться\n",
      "['M-PREF', 'M-PREF', 'E-PREF', 'M-ROOT', 'M-ROOT', 'M-ROOT', 'E-ROOT', 'S-SUFF', 'M-SUFF', 'E-SUFF', 'S-POSTFIX', 'S-POSTFIX']\n",
      "['B-PREF', 'M-PREF', 'E-PREF', 'B-ROOT', 'M-ROOT', 'M-ROOT', 'E-ROOT', 'S-SUFF', 'B-SUFF', 'E-SUFF', 'B-POSTFIX', 'E-POSTFIX']\n",
      "Количество классов классификации в \"новой\" модели: 19\n",
      "Количество классов классификации в \"новой\" модели: 25\n"
     ]
    }
   ],
   "source": [
    "print(inputs[2])\n",
    "print(predicted_targets_new[2][0])\n",
    "print(predicted_targets_old[2][0])\n",
    "print('Количество классов классификации в \"новой\" модели: ' + str(len(predicted_targets_new[2][1][0])))\n",
    "print('Количество классов классификации в \"новой\" модели: ' + str(len(predicted_targets_old[2][1][0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка всех вариантов модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка предобученной модели как есть (загруженной из GIT проекта):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_raw = measure_quality(targets, [elem[0] for elem in predicted_targets_raw],\n",
    "                                  english_metrics=params.get(\"english_metrics\", False),\n",
    "                                  measure_last=measure_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера=98.00\n",
      "Корректность=96.29\n",
      "Полнота=97.98\n",
      "Точность=98.02\n",
      "Точность по словам=84.36\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(quality_raw):\n",
    "            print(\"{}={:.2f}\".format(key, 100*value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка предобученной модели без использования исправляющего алгоритма _decode_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_last = params.get(\"measure_last\", use_morpheme_types)\n",
    "quality = measure_quality(targets, [elem[0] for elem in predicted_targets],\n",
    "                                  english_metrics=params.get(\"english_metrics\", False),\n",
    "                                  measure_last=measure_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера=98.00\n",
      "Корректность=96.45\n",
      "Полнота=97.99\n",
      "Точность=98.00\n",
      "Точность по словам=87.99\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(quality):\n",
    "            print(\"{}={:.2f}\".format(key, 100*value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка модели, обученной на сокращённом формате разметки BMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера=97.50\n",
      "Корректность=96.18\n",
      "Полнота=97.45\n",
      "Точность=97.56\n",
      "Точность по словам=81.20\n"
     ]
    }
   ],
   "source": [
    "measure_last = params.get(\"measure_last\", use_morpheme_types)\n",
    "quality_new = measure_quality(targets2, [elem[0] for elem in predicted_targets_new],\n",
    "                                  english_metrics=params.get(\"english_metrics\", False),\n",
    "                                  measure_last=measure_last)\n",
    "for key, value in sorted(quality_new):\n",
    "            print(\"{}={:.2f}\".format(key, 100*value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка модели, обученной заново в неизменном виде "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера=97.56\n",
      "Корректность=95.46\n",
      "Полнота=97.54\n",
      "Точность=97.57\n",
      "Точность по словам=79.91\n"
     ]
    }
   ],
   "source": [
    "quality_old = measure_quality(targets, [elem[0] for elem in predicted_targets_old],\n",
    "                                  english_metrics=params.get(\"english_metrics\", False),\n",
    "                                  measure_last=measure_last)\n",
    "for key, value in sorted(quality_old):\n",
    "            print(\"{}={:.2f}\".format(key, 100*value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохранение моделей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_new.to_json(\"models_lpnsk/morphemes-1-1.json\", \"models_lpnsk/morphemes-model-1-1.hdf5\")\n",
    "cls_old.to_json(\"models_lpnsk/morphemes-1-2.json\", \"models_lpnsk/morphemes-model-1-2.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
